# 强化学习
AlphaGo Zero中的深度神经网络，使用强化学习算法在自我对弈的过程中进行训练。对于每个棋面 ![](http://latex.codecogs.com/svg.latex?s), 使用前一轮得到的神经网络 ![](http://latex.codecogs.com/svg.latex?f) 来指导MCTS。MCTS会输出每一步走棋的概率分布 **π**。搜索概率 π 比单纯的神经网络输出 ![](http://latex.codecogs.com/svg.latex?p) 概率分布更好，能够选择更强的走棋。从这个角度来说，MCTS可以看做是 Policy Iteration 算法中的策略提升操作（policy improvement operator）。这意味着，在自我对弈过程中，不断使用提升过的、基于 MCTS 的策略 π 来进行决策走棋，最终会决出胜负，将最终的获胜方z作为一个样本值，可以看做是 Policy Iteration 中的策略评估操作（policy evaluation operator），即对当前棋面局势的评估，作为当前棋面下，获胜概率的预测值![](http://latex.codecogs.com/svg.latex?v)。这里强化学习算法的关键是在 Policy Iteration过程中，不断重复地使用 policy improvement 和 policy evaluation。蒙特卡罗搜索结束后，更新 ![](http://latex.codecogs.com/svg.latex?f) 的参数θ，使得更新完的 ![](http://latex.codecogs.com/svg.latex?f) 的输出：移动概率和预测值 ![](http://latex.codecogs.com/svg.latex?(p,v)=f(s))，能够更加接近MCTS得到的搜索概率和自我对弈的获胜方(π,z)。这个新的 ![](http://latex.codecogs.com/svg.latex?f) 将在下一轮自我对弈中继续指导MCTS，使得其变得更健壮。

<div align=center>
<img src="../images/RL.png" width = "737" height = "263" align=center/>
</div>

## 1 数据产生过程
对于 ![](http://latex.codecogs.com/svg.latex?t) 时刻下的棋盘状态 ![](http://latex.codecogs.com/svg.latex?s_t)，MCTS 可以模拟得到下一步落子的概率分布 **π**，不断对弈一直分出胜负得到回报分数 ![](http://latex.codecogs.com/svg.latex?r_T)，(![](http://latex.codecogs.com/svg.latex?T>t))，将这个分数反馈到了每一个 ![](http://latex.codecogs.com/svg.latex?t) 的 ![](http://latex.codecogs.com/svg.latex?z_t) 上，且满足 ![](http://latex.codecogs.com/svg.latex?z_t=+r_T/-r_T) （正负号取决于当前对局最终的获胜者），那么我们就得到了一组训练数据(![](http://latex.codecogs.com/svg.latex?s_t),πt,![](http://latex.codecogs.com/svg.latex?z_t))，一场比赛会产生很多训练数据，我们将 ![](http://latex.codecogs.com/svg.latex?s_t) 作为 ![](http://latex.codecogs.com/svg.latex?f) 的输入，(πt,![](http://latex.codecogs.com/svg.latex?z_t))作为标签，使得 ![](http://latex.codecogs.com/svg.latex?f) 的输出(![](http://latex.codecogs.com/svg.latex?p),![](http://latex.codecogs.com/svg.latex?v))趋近于(πt,![](http://latex.codecogs.com/svg.latex?z_t))。![](http://latex.codecogs.com/svg.latex?z_t) 相当于告诉 AI 应该这盘对弈中应该强化那些落子决策，以及弱化那些落子决策。

## 2 决策优化过程
![](http://latex.codecogs.com/svg.latex?f) 首先使用随机权重 θ 进行初始化。在后续每一轮迭代 ![](http://latex.codecogs.com/svg.latex?i) 中（每轮迭代对应若干局完整的围棋对弈），![](http://latex.codecogs.com/svg.latex?f) 使用强化学习算法 Policy Iteration 进行训练优化，该强化学习算法将 MCTS 纳入训练过程，用前一轮的神经网络 ![](http://latex.codecogs.com/svg.latex?f)(θi-1)在该轮迭代i的每个时间步 ![](http://latex.codecogs.com/svg.latex?t) 来指导 MCTS 模拟得到 πt=α(θi−1,![](http://latex.codecogs.com/svg.latex?s_t))，直到时间步T产生胜负，按照上述描述（**数据产生过程**）来构造该轮对弈产生的数据(![](http://latex.codecogs.com/svg.latex?s_t),πt,![](http://latex.codecogs.com/svg.latex?z_t))。接着，![](http://latex.codecogs.com/svg.latex?f)(θi)使用该训练数据进行参数更新，目标是使得新的 ![](http://latex.codecogs.com/svg.latex?f)(θi+1)的输出(![](http://latex.codecogs.com/svg.latex?p),![](http://latex.codecogs.com/svg.latex?v))= ![](http://latex.codecogs.com/svg.latex?f)(θi+1,![](http://latex.codecogs.com/svg.latex?s))能够拟合(π,![](http://latex.codecogs.com/svg.latex?z))，也即最小化 ![](http://latex.codecogs.com/svg.latex?v) 和 ![](http://latex.codecogs.com/svg.latex?z) 之间的误差，最大化 ![](http://latex.codecogs.com/svg.latex?p) 和 π 之间的相似性。优化损失函授结合了 MSE 和 Cross-Entropy Loss（还包括正则化项）


